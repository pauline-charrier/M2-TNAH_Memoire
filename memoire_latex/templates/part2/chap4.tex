L’un des objectifs principaux et réaffirmé des acteurs de \pense côté numérique est le décloisonnement entre ingénieurs et chercheurs : il ne s’agit pas de limiter le numérique à la réalisation isolée d’interfaces de valorisation, mais au contraire d’inviter les chercheurs à s’y intéresser et à y contribuer pleinement – sensibiliser le monde de la recherche, d’autant plus dans le domaine de l’histoire de l’art. Ce positionnement, tendant volontiers vers une discussion politique, en opposition avec l’idée selon laquelle une technologie efficace serait nécessairement une « boîte noire », c’est-à-dire un système complexe et opaque, voire hermétique dont les mécanismes demeuraient inaccessibles aux non techniciens, est au centre de la philosophie revendiquée par le projet, et en dicte plusieurs approches. 

\subsection{Une stratégie revendiquée : « approche web » contre « approche applicative »}

\subsubsection{Recherche d’accessibilité contre quête de performance}

Cette opposition entre « approche web » et « approche applicative » est comprise dans le contexte présent comme une opposition entre :  

\begin{itemize}[label=\textbullet] % premier niveau de la liste (point rond)
    \item une démarche valorisant les technologies dites du « web », à savoir des technologies (langages et formats), 
    \begin{itemize}[label=--] % deuxième niveau de la liste (tiret)
        \item permettant de structurer et d’exposer des données hébergées sur le web 
        \item accessibles par l’intermédiaire d’un navigateur,
        \item généralement installées depuis plusieurs années dans le paysage web et dont l’histoire est souvent intimement liée au développement de ce dernier 
        \item appuyées sur des standards établis et partagés, 
        \item faisant l’objet de spécifications par le World Wide Web Consortium, organisme de standardisation assurant une certaine garantie d’interopérabilité quel que soit le navigateur et le système d’exploitation utilisé pour accéder aux contenus ; 
    \end{itemize}
    \item une démarche apparaissant davantage : 
    \begin{itemize}[label=--]
            \item tournée vers l’indépendance par rapport au web et aux navigateurs, permettant de concevoir des applications tournant directement un système d’exploitation par exemple,
            \item comme une approche valorisant plutôt l’innovation et la performance et incorporant volontiers une dimension commerciale dont cherchent à s’écarter les technologies traditionnelles du web. 
    \end{itemize}
\end{itemize}

Cette opposition, dont les contours demeurent parfois flous, est particulièrement perceptible dans le discours tenu par l’équipe du projet et dans les choix technologiques réalisés. 
Ainsi, comme l’affirme Jean-Christophe Carius, si des solutions technologiques relativement récentes comme les \textit{frameworks} « tout-JavaScript » (tels que React ou Vue) – assimilé ici à une technologie « non-web », ce qui peut être discutable sur certains points, JavaScript étant généralement groupé dans la catégorie des technologies « web », y compris sous sa forme de langage de backend – sont fréquemment plébiscités dans le monde industriel en raison de leur capacité à gérer des applications à forte charge et à grande échelle (un concept que l’on désigne par le terme de « scalabilité »), il est pertinent de s'interroger sur leur choix pour des projets d’humanités numériques, qui ne partagent a priori pas les mêmes besoins, dans l’écrasante majorité des cas.  On observe parfois une attirance (peut-être par une forme de mimétisme) pour ces outils dans le monde académique, au détriment de technologies web pourtant jugées plus accessibles, plus pérennes et tout aussi efficaces pour les besoins des chercheurs en humanités. 
Cette adoption vue comme « excessive » par certains acteurs du projet, de solutions industrielles dans un contexte de recherche académique peut être questionnée : quelle est la véritable valeur ajoutée de ces outils complexes lorsque l’on travaille sur des problématiques propres aux sciences humaines et sociales ? N’y a-t-il pas pourtant un risque de perte d’intelligibilité, de trop grande complexité des outils et des contenus ainsi développés ?  Un risque d’isolation de l’ingénieur par rapport au chercheur ?
S’inscrivant en faux contre cette tendance (qui demeure cependant encore modeste dans le monde des humanités numériques francophones), \pense tend à entretenir une philosophie de relative simplicité quant à ses choix technologiques, mettant en avant l’accessibilité et la transparence et rejetant une certaine complexité perçue comme inutile. Le recours à des technologies web « simples » et ancrées depuis longtemps dans le paysage d’Internet (\html, CSS…) permet d’inclure un plus grand nombre de chercheurs, qui ont plus de chances d’y avoir été familiarisés. Cela ne signifie pas pour autant un refus complet de la technologie avancée, mais un recentrage sur des outils adaptés aux besoins spécifiques de la recherche.

Cette approche rejoint très partiellement celle du mouvement « low-code », voire « no-code », une tendance qui vise à proposer des outils accessibles sans nécessiter des compétences en programmation. Si ce mouvement suscite parfois des critiques (sécurité, accusation d’ « illettrisme numérique » \footcite[p.3]{cahier_comment_2022}, etc.), il n’en reste pas moins une voie intéressante pour l’inclusion de chercheurs non techniciens dans des projets numériques. Un exemple probant dans le domaine des humanités numériques est l’outil \textit{ImageGraph}, développé par Leonardo Impett, qui permet de manipuler des graphes d’images sans avoir à écrire de code complexe\footnote{Voir l’URL suivante : https://www.meshs.fr/page/imagegraph cité dans \footcite{aubry_artificial_2021}}, cité dans Aubrey et al. Le cas des \cms ou logiciels de gestion de contenu, dont font partie Wordpress, Drupal et Omeka est un autre exemple plus largement utilisé dans le domaine des SHS pour construire et maintenir des sites web à moindre coût.
Le mouvement no-code offre une réponse pour le moins radicale à une problématique récurrente dans le domaine des humanités numériques : celle de savoir si les chercheurs en humanités numériques doivent ou non apprendre à coder. Comme le souligne Consoli (cité par \citeauthor{anderson_teaching_nodate}), ce débat divise la communauté des humanités numériques. Certains estiment en effet qu’il est contre-productif pour les chercheurs de s’investir dans l'apprentissage du code, arguant que cela détourne leur attention de leur véritable objectif scientifique\footcite{anderson_teaching_nodate}.

\subsubsection{S’inscrire dans les principes démocratiques du Web}

Le choix de recourir à une application web plutôt qu’à une application indépendante du Web s’inscrit dans une vision qui privilégie l’ouverture et l’accessibilité. Comme le stipulent les principes éthiques du Web, « \textit{the web is for all people} », et l’utilisation du Web ne devrait pas, selon ces principes, nécessiter un niveau élevé de compétences techniques. En ce sens, les technologies web se doivent d’être « intelligibles » et « ouvertes » afin de permettre une appropriation par un large public, sans exiger des compétences en développement\footcite{noauthor_ethical_nodate}. 
Cette démarche fait aussi partiellement écho aux initiatives d’ouverture des données dans le cadre de la recherche scientifique (accessibilité, circulation de l’information). En France, ces principes sont fortement encouragés, notamment avec la promotion des principes \fair \footcite{wilkinson_fair_2016}. Ces principes, en phase de diffusion dans la communauté scientifique internationale, visent à garantir que les données produites dans le cadre de la recherche soient facilement trouvables, accessibles à tous, interopérables avec d’autres systèmes, et réutilisables par d’autres chercheurs.

Le projet \pense s’inscrit dans cette logique d’ouverture, cherchant à favoriser la circulation des connaissances à travers des technologies web accessibles. Une ambition entérinée au niveau institutionnel avec la diffusion de la \textit{Charte pour l’ouverture des données de l’Institut national d’histoire de l’art}\footcite{nurra_pratique_2023}.

Cependant, notons que cette ouverture des données n’est pas sans coût. Comme le note plusieurs auteurs, ouvrir les données implique souvent des processus longs et coûteux pour les chercheurs et les ingénieurs, notamment en termes d’anonymisation et de gestion des aspects légaux\footcite[p.60]{aubry_artificial_2021}. Ce défi est particulièrement sensible dans les sciences sociales, où il est parfois difficile de concilier ouverture et protection des données sensibles et de doser entre « suffisamment ouvert » et « suffisamment fermé » \footcite{bendjaballah_sciences_2023}, phénomène également abordé par Etienne Anheim qui souligne la surcharge de tâches non directement académiques\footnote{Anheim parle cependant ici spécifiquement de tâches administratives qui ne relèvent pas de la recherche académique à proprement parler.} dans le quotidien des chercheurs\footcite{anheim_administrer_2018}. Il faut cependant noter que cet aspect ne concerne pas directement le cadre de \pense, du fait de la relative ancienneté des sources traitées.

\subsection{Une certaine prise de distance avec les procédés d’automatisation}

\subsubsection{Préserver « le goût de l’archive » (Arlette Farge) : importance du recours aux tâches manuelles}\footnote{\textit{Le Goût de l'archive} est le titre d’un ouvrage très fréquemment commenté d’Arlette Farge (1989) traitant de la relation intime entre le chercheur en histoire et les sources primaires}

Le concept de « goût de l’archive » proposé par l’historienne Arlette Farge, dans l’ouvrage éponyme publié en 1989, souligne l’importance du lien intime entre le chercheur et l’archive dans sa matérialité, dans sa dimension physique, tangible et souvent fragile. Un lien repensé (mais non pas écarté\footcite{clavert_gout_nodate}) à l’ère du numérique avec la transformation de l’archive en « donnée », traitable non seulement plus par l’humain, mais par la machine, installant une médiation supplémentaire qui n’existait pas auparavant.
\newline
\textbfit{Transcription manuelle contre HTR : raisons scientifiques, logistiques et organisationnelles d’une préférence}\\

La transcription manuelle demeure, pour nombre de spécialistes, une activité fondamentale du travail sur les sources. Elle n’est pas simplement une étape technique de reproduction d’un texte, mais représente un véritable travail d’analyse et d’appropriation critique. Comme le font remarquer plusieurs auteurs dans des contextes différents, la transcription est déjà en elle-même une forme d’analyse scientifique, s’inscrivant pleinement dans le travail du chercheur\footcite[p.80]{dufournaud_humanites_2014}, complémentant le travail de commentaire critique (qui peut la suivre ou la précéder) et permettant une véritable « appropriation » du texte, rapprochant la source du chercheur, et dont l’encodage est la suite logique\footnote{« L’encodeur est l’accoucheur du texte » selon Frédéric Duval, dans \footcite[p.15]{duval_pour_2017}}. 
Ce processus est couramment vu auprès des chercheurs non techniciens comme une alternative plus enrichissante que la simple correction d’erreurs issues d’une sortie \ocr ou \htr (reconnaissance par la machine des caractères ou de l’écriture manuscrite), tâche considérée comme aliénante car réduisant le chercheur à un rôle de correcteur typographique, limitant l'interaction avec le contenu à une dimension mécanique et non analytique. Certains chercheurs se détournent aussi des technologies de reconnaissance optique des caractères, non seulement pour des raisons scientifiques, mais aussi parce que ces technologies ne sont pas toujours fiables ou adaptées à des corpus trop peu volumineux ou trop complexes. C’est par exemple le cas du Fonds Thierry, que nous avons déjà évoqué, où la reconnaissance de caractères a échoué en raison de la présence de graphies non latines que l’\ocr n’a pas su déchiffrer. Ce cas illustre les limites de l’automatisation, qui nécessite, même dans le meilleur des cas, une intervention humaine (correction) pour garantir une transcription fiable\footcite{chague_intelligence_2022}.

La méfiance envers l'automatisation dans la transcription se reflète également dans le rejet de ce que certains perçoivent comme une « industrialisation »\footcite[p.83]{chateau-dutier_editions_2021} du processus éditorial. Selon certaines critiques, la chaîne automatisée, qui peut s’étendre de la segmentation à la transcription en passant par le balisage, a pour effet de distancier le chercheur de la source et de transformer une activité intellectuelle en une simple procédure de production. Ce processus est perçu comme l'antithèse de l’approche attentive et critique habituellement attendue dans l’étude des archives.
D’autre part, certaines méthodes de transcription plus spécifiques, comme la transcription imitative, qui consiste à reproduire fidèlement les particularités d’un texte, y compris ses erreurs orthographiques et grammaticales, ont également suscité des débats. Cette méthode, envisagée dans le cadre du projet d'édition numérique des archives Doucet, repose sur une volonté scientifique de fidélité maximale au texte d’origine (édition diplomatique). Toutefois, même des chercheurs expérimentés, (dont des chercheurs ayant travaillé auprès de \pense et dont les propos ont été recueillis par l’intermédiaire de l’ingénieur en charge du projet) ont souligné la difficulté qu’il peut y avoir à produire manuellement un texte imitatif fidèle. En effet, il n'est pas aisé pour un scripteur formé aux normes orthographiques et syntaxiques contemporaines de respecter consciemment les fautes et variations présentes dans un texte d'époque, ce qui complique la réalisation d'une transcription véritablement « authentique » et rend la tâche peut-être plus pénible.
Par ailleurs, en dépit de l’intérêt scientifique que peut représenter la transcription humaine, d'autres solutions sont parfois envisagées pour pallier le caractère chronophage de la transcription manuelle par les chercheurs, mais ces dernières présentent elles-mêmes des limites importantes. L’externalisation vers des prestataires professionnels ou le recours à des amateurs bénévoles dans des initiatives de crowdsourcing sont des pistes souvent étudiées\footcite{chague_intelligence_2022}. Néanmoins, ces alternatives comportent également leurs propres défis, soulignés par \citeauthor{chague_intelligence_2022}, que ce soit en termes de coûts ou de contrôle qualité.
Dans le cas précis des archives Doucet, la fidélité au texte original, qui a contribué à orienter la décision vers une transcription manuelle, s’inscrit dans une perspective d’édition « diplomatique », c’est-à-dire une reproduction la plus exacte possible des documents dans leur forme d’origine\footcite{masai_principes_1950}, y compris les erreurs et anomalies\footcite{gvelesiani_quest-ce_2017}. Cette approche est jugée essentielle dans le cadre de l’édition critique, car elle permet de restituer au plus près la forme du texte avant toute intervention éditoriale\footcite[p.18]{duval_pour_2017}. Toutefois, l’édition Doucet se distingue par sa double ambition. D’une part, elle vise à produire une édition de type diplomatique, fidèle en tous points à l’orthographe (parfois fautive) et à la mise en forme d’origine, d’autre part, elle cherche à proposer une édition critique plus « traditionnelle », comprenant des corrections permettant la pleine intelligibilité du texte\footcite{carius_principes_2024}.
\newline
\textbfit{Une possible méfiance vis-à-vis de l’informatique ?}\\

Il est important de souligner que le projet \pense ne prend pas parti sur l’usage ou non des technologies d’automatisation, mais cherche avant tout à s’adapter aux besoins des chercheurs. La dimension exploratoire est présente, notamment dans les essais d’automatisation des processus de correction après une première phase de transcription imitative, nous le verrons dans \hyperlink{chap7}{le chapitre 7}, mais demeure balisée par l’objectif principal du projet, qui reste centré sur les attentes scientifiques des équipes de recherche. 
Enfin, une certaine méfiance persiste vis-à-vis de l’informatique en général. Comme le rappelle Berra, ce scepticisme repose en partie sur des « représentations sociales » ambivalentes de l’informatique. D’un côté, elle est perçue comme porteuse de progrès et de liberté, mais de l’autre, elle est aussi vue comme un vecteur de bureaucratisation et de déshumanisation\footcite[p.614]{berra_pour_2015}. 
Certaines critiques dénoncent également ce qu’elles appellent le \textit{technological hipsterism} (Earhart), c’est-à-dire une tendance à privilégier l’innovation technologique pour elle-même (« \textit{innovation for innovation’s sake} », comme le formule \citeauthor{earhart_digital_2012}), au profit d’une recherche effrénée de nouveauté et au détriment de la qualité scientifique du travail\footcite[p.24]{earhart_digital_2012}. Le projet \pense se distance clairement et explicitement de cette approche, privilégiant la rigueur scientifique sur l’innovation pour l’innovation. 

\subsubsection{Frilosités institutionnelles possibles vis-à-vis de l’intelligence artificielle}

Le recours à l’\ia dans le domaine de l’automatisation des processus de transcription ou d’analyse textuelle soulève des questions à la fois techniques et politiques. À une époque où l’IA générative semble accessible à tous, il paraît naturel de vouloir intégrer ces technologies dans le champ de la recherche. Cependant, cette intégration soulève des préoccupations, notamment en ce qui concerne la souveraineté des données et des outils utilisés, et plus largement la dépendance possiblement induite à des entreprises privées étrangères pour l’accès à des jeux de données ou à des outils.
\newline
\textbfit{Enjeux politiques : souveraineté des données et du code}\\

L’un des premiers aspects à relever concerne la compréhension du réel fonctionnement des outils d’IA disponibles pour la recherche (crainte d’une « boîte noire ») et la question de l’ouverture des données, une dimension plutôt essentielle pour la mise en place de protocoles de recherche, comprenant une réflexion sur la réplicabilité des expériences, par exemple. L’une des distinctions à effectuer lorsque l’on parle de transparence dans le domaine de l’IA concerne la différence entre l’ «\textit{open source} », qui désigne une transparence totale du code, et les « \textit{open weights} », qui n’ouvrent qu’une partie des paramètres d’entraînement d’un modèle d’IA (typiquement un grand modèle de langage) sans en rendre public le code complet\footcite{ramlochan_openness_2023}. Un certain nombre de \llm à l’heure actuelle relèvent de l’open source, comme les modèles LLAMA de Meta, Mistral de Mistral AI ou BERT de Google, tandis que d’autres demeurent propriétaires, comme GPT d’OpenAI. 
Si ces approches apportent une très relative transparence, elles ne freinent pas les craintes quant aux questions de souveraineté, qui se posent, notamment pour des applications dans des domaines stratégiques\footcite[p.77]{pollotec_intelligence_2018}. Les applications dans le monde de la culture, qui ne relève pas à proprement parler d’un secteur « stratégique » au sens où l’entend traditionnellement le monde politique (qui voit plutôt dans cette appellation des domaines régaliens comme la défense ou le maintien de l’ordre public), sont tout de même soumises à des enjeux de souveraineté proche, notamment en ce qui concerne une possible dépendance aux grandes industries technologiques.
Comme le souligne Sauret, les chercheurs en sciences humaines et sociales (SHS) doivent veiller à constituer leurs propres jeux de données et à éviter de se reposer exclusivement sur les ressources fournies par les entreprises privées, sous peine d'introduire des biais parfois majeurs dans leurs travaux\footcite[p.102]{sauret_intelligence_2022} (d’autant plus susceptibles de passer sous les radars dans le cas d’une « boîte noire » telle qu’évoquée plus haut), entraînant des répercussions épistémologiques significatives et pas forcément anticipées.
\newline
\textbfit{Enjeux budgétaires}\\

Un autre enjeu de taille réside dans les infrastructures nécessaires à l’entraînement des modèles d’IA, qui nécessitent d’importantes capacités de calcul (usage de \gpu). Des solutions de mutualisation des ressources, comme le supercalculateur Jean Zay en France, permettent de réduire les coûts tout en garantissant des performances optimales. Au-delà des ressources informatiques massives à mobiliser, un autre aspect coûteux concerne la constitution et le rassemblement de corpus d’entraînement, données d’apprentissage et d’évaluation des modèles\footcite{chague_lintelligence_2022}. 
\newline
\textbfit{Enjeux éthiques}\\

Enfin, l’utilisation de l’IA pose des questions éthiques non négligeables, tant sur le plan écologique que social. D’un point de vue environnemental, l'entraînement des modèles d’IA entraîne une empreinte carbone importante, il apparaît donc essentiel de développer ou de privilégier des modèles plus « sobres » et moins gourmands en énergie\footcite[p.3]{chague_lintelligence_2022}, ce qui implique parfois des sacrifices budgétaires que les institutions patrimoniales ou de recherche ne peuvent pas nécessairement toujours se permettre. Sur le plan social, l’exploitation des travailleurs précaires, notamment dans les pays des Suds, pour entraîner ces modèles, comme le montre le cas des plateformes telles qu’\textit{Amazon Mechanical Turk}\footnote{Voir à ce sujet l’ouvrage de Phil Jones, \textit{Work Without The Worker}, 2021.}, soulève également des questions de justice sociale. Si la recherche souhaite s’appuyer sur des ressources, il apparaît essentiel de considérer ces aspects avant de s’engager dans une démarche d’adoption de ces outils.

\subsection{La technologie au service de la recherche}

\subsubsection{L’importance du dialogue ingénieur/chercheur}

L’usage des technologies numériques au sein des sciences humaines et sociales a considérablement modifié les pratiques de recherche. En particulier, l’émergence des humanités numériques a engendré une collaboration de plus en plus étroite entre chercheurs et ingénieurs. Ces derniers, souvent perçus comme des facilitateurs, jouent désormais un rôle central dans la production des connaissances, en tant que véritables partenaires. Cette section se penche sur la manière dont le projet \pense illustre cette complémentarité entre technologie et recherche. En nous appuyant sur des exemples concrets issus de notre expérience de stage, nous analyserons l’importance du dialogue entre ingénieurs et chercheurs ainsi que la manière dont la technologie peut s’articuler avec le travail scientifique.
En France, une distinction claire entre ces deux rôles subsiste, en partie dû à un relatif cloisonnement administratif, ce qui n’est pas toujours le cas à l’étranger, comme le soulignent Florence Clavaud et Aurélien Berra\footcite{collectif_formations_2012}. Entre ces deux rôles considérés culturellement et administrativement comme distincts et incarnés dans des individualités séparées, la nécessité du dialogue est mise en évidence pour la bonne mise en œuvre de projets en humanités numériques.

Ce dialogue se concrétise, par exemple, dans le cadre du projet \pense, par des réunions régulières mises en place entre les ingénieurs et les chercheurs responsables des différents volets du projet, un point majeur régulièrement abordé touche à la question des compromis à adopter entre ingénieur et chercheur (qui se traduit souvent sous la forme compromis entre intelligibilité et « performance » notamment). Un exemple significatif de cette collaboration est la question de la gestion des formats de données. Alors que les ingénieurs privilégient généralement l’usage de formats ouverts et interopérables, tels que le \csv, les chercheurs, pour des raisons de confort, préfèrent souvent utiliser des formats propriétaires comme Excel, affichant une certaine ergonomie. Afin de faciliter ce dialogue, une solution technique a été mise en place pour permettre aux chercheurs d’accéder à leurs données sous le format qu’ils maîtrisent le mieux. Ainsi, un bouton a été ajouté sous la prototype de chronologie évoquée dans \hyperlink{chap3}{le chapitre 3}, permettant à la chercheuse de télécharger son fichier de travail au format Excel et non \csv. Ce fichier est converti depuis le format \csv par un script Python utilisant la bibliothèque Pandas. Ce script permet de lire les données sous forme de \textit{dataframe} (tableau à deux dimensions) et de les exporter dans un fichier Excel grâce à la méthode `dataframe.to\_excel`. Par la suite, ce fichier est inséré dans le module XQuery décrivant la page web de la visualisation chronologique, afin de rendre le fichier téléchargeable sous la forme d’un lien.

Ce dialogue entre ingénieurs et chercheurs ne s’arrête pas à des ajustements techniques ponctuels, mais s’inscrit dans une réflexion plus globale sur la manière dont ces deux mondes peuvent travailler de concert. Comme le remarque \citeauthor{bonfait_humanites_2021}, « il faut donc créer des processus d’expérimentations impliquant des spécialistes des deux domaines, histoire de l’art et humanités numériques, pouvant dialoguer sur un même objet de recherche ou inventer des espaces d’interactions » \footcite[p.8]{bonfait_humanites_2021}. Ce type d’expérimentation encourage la collaboration plutôt que la compartimentation des savoirs. En engageant un dialogue constant, ingénieurs et chercheurs dépassent la simple cohabitation de leurs compétences pour s’engager dans un processus collectif de production de connaissances. Ce changement de paradigme, favorisé par l’émergence des humanités numériques, implique une complémentarité entre ingénieurs et chercheurs, qui ne travaillent plus de manière isolée\footnote{Elément mentionné dans \footcite[p.3]{massot_dessiner_2018} et \footcite[p.172]{pelissier_accompagner_2017}}.     

Dans cette optique, l’ingénieur ne peut plus être perçu comme un simple auxiliaire technique\footcite[p.5]{massot_dessiner_2018}. Grâce à ses connaissances techniques, il guide le chercheur dans la mise en œuvre des outils numériques et contribue activement à la production scientifique\footcite[p.7]{massot_humanites_2021}  .

\subsubsection{La technologie complémentaire du travail du chercheur}

La collaboration entre ingénieurs et chercheurs ne signifie pas pour autant que la technologie doive remplacer le travail de recherche\footcite[p.453]{graham_introduction_2017}. En réalité, elle vient en complément des efforts scientifiques. Comme le rappelle \citeauthor{chateau-dutier_editions_2021}, « dans l’idéal, l’activité du chercheur doit le plus possible pouvoir être accueillie dans l’interface numérique » \footcite[p.86]{chateau-dutier_editions_2021}. L’objectif des outils technologiques n’est pas de se substituer au travail du chercheur, mais bien de le faciliter, en permettant une intégration fluide des méthodes de recherche au sein des interfaces numériques.
Le projet \pense s’inscrit dans cette philosophie, en évitant la « prévalence de la technologie sur les besoins de la recherche » \footnote{« L’édition numérique de sources historiques enrichies nous a montré qu’il faut garantir l’équilibre entre les humains et les machines, en évitant la prévalence de la technologie sur les besoins de la recherche. Il faut savoir mettre à disposition des usagères et des usagers des outils numériques qui soient faciles à utiliser, sans jamais banaliser la portée scientifique des contenus. Le but principal est donc de familiariser les chercheuses et les chercheurs avec les outils et les pratiques numériques et leur permettre ensuite d’intégrer ces outils dans la méthodologie de recherche, afin de garantir l’exploitation ainsi que l’intégrité scientifique des données produites » in \footcite{colonna_digital_2024}}. Il s’agit avant tout de créer un environnement où les outils numériques sont mis au service des besoins des chercheurs, sans jamais étouffer leur démarche scientifique. Cette approche est d’ailleurs au cœur du travail du Service numérique de la recherche (\snr) de l’\inha, qui cherche à équilibrer les contributions humaines et technologiques.

Cette philosophie se retrouve dans des aspects très concrets du projet, notamment dans la production de documents intermédiaires à l’usage exclusif de la recherche. Par exemple, le \textit{teiCorpus} et les index sont des outils créés pour permettre aux chercheurs de manipuler des données avant leur publication finale. Les index offrent une vue d’ensemble des noms de personnes, de lieux, d’organisations et d’œuvres que le balisage \tei a permis d’identifier. Cela permet aux chercheurs de détecter d’éventuelles incohérences ou erreurs de capture et de compléter les informations en vue de l’enrichissement des balises par des attributs spécifiques.
De même, le \textit{teiCorpus} avec indexation interne est un fichier unique rassemblant l’ensemble des fichiers \tei de la base de données, permettant une visualisation globale des contenus. Cet outil permet aux chercheurs de n’avoir à consulter qu’un seul index pour avoir accès à l’ensemble des données et de vérifier la cohérence des informations entre les différents fichiers. Ces productions intermédiaires, bien que n’étant pas destinées à être publiées dans le cadre de l’édition numérique finale, jouent un rôle éminent dans le processus de validation des données et d’enrichissement des corpus. Elles illustrent ainsi l’articulation complémentaire entre la technologie et le travail scientifique dans le cadre du projet \pense.