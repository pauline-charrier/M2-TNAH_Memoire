L’intelligence artificielle (IA), notamment sa dimension générative, fait l’objet d’un intérêt croissant depuis l’avènement des grands modèles de langage génératifs, comme celui développé par OpenAI avec ChatGPT, qui facilitent considérablement les interactions entre utilisateurs et modèles d’IA. Bien que ces systèmes aient contribué à une médiatisation intense et à une certaine « démocratisation » de l’IA, il est important de souligner que celle-ci ne se limite pas à ces applications spécifiques. En effet, l’intelligence artificielle représente un champ bien plus vaste (absolument non restreint à l’IA neuronale dont les modèles génératifs cités plus hauts tendent à devenir des emblèmes bien connus du grand public) et ancré dans une histoire longue et complexe. 
Au sein des humanités numériques, l’usage de l’IA se concentre essentiellement sur l’analyse textuelle (dont traitement automatique de la langue) et l’analyse d’image (forme du traitement du signal\footcite[p.98]{sauret_intelligence_2022}), qui figurent parmi les quatre principaux piliers de ce que Olivier Bonfait et ses collaborateurs appellent la « \textit{computational art history}\footcite[p.6]{bonfait_humanites_2021}, une approche qui s’inscrit dans un cadre méthodologique où les technologies numériques transforment la recherche en histoire de l’art. 

\subsection{Histoire (non exhaustive) de l’intelligence artificielle}
\subsubsection{La naissance d’un concept}

La notion d’intelligence artificielle, telle que nous la comprenons aujourd’hui, a pris forme dès le milieu du XXe siècle. Marvin Minsky, l’un des pionniers du domaine, décrivait l’IA comme la « construction de programmes informatiques qui s’adonnent à des tâches qui sont, pour l’instant, accomplies de façon plus satisfaisante par des êtres humains car elles demandent des processus mentaux de haut niveau tels que l’apprentissage perceptuel, l’organisation de la mémoire et le raisonnement critique »\footcite{puren_intelligence_2020}. 
Dans la même veine, Yann LeCun, autre figure de l’IA, précise que son objectif est de « faire faire aux machines des activités que l’on attribue généralement […] aux humains »\footcite{saporta_breve_2018}. 
Ces définitions qui soulignent d’abord le caractère imitatif de l’IA avant la notion de dépassement des capacités humaines, entrent en écho avec une question féconde dans le domaine de la science-fiction, source de fantasmes métaphysiques et de réflexions théoriques sur la nature-même de l’être humain, présentes dans beaucoup de cultures (de « l’homme machine » de La Mettrie (1747) au personnage du « golem » dans le folklore juif…) – l’angoisse et l’espoir simultanés qu’inspire l’IA reflètent en partie la tension entre le pouvoir créateur de l’humain et ses limites, voire son impuissance vis-à-vis d’une potentielle future autonomie des machines. 
Le terme « intelligence artificielle » lui-même a été formulé en 1956 par John McCarthy\footcite{bermes_patrimoine_2020} durant un atelier organisé à Dartmouth College et réunissant des scientifiques pionniers de l’IA tels que Minsky, Allen Newell et Claude Shannon\footcite{puren_intelligence_2020}. 
\newline
\textbfit{IA forte et IA faible}\\

La distinction entre IA « forte » et IA « faible » est fondamentale pour comprendre les différentes approches de cette technologie. L’IA forte désigne une machine capable de développer une véritable conscience de soi, de ressentir des émotions authentiques et de produire un raisonnement lui permettant de comprendre les raisons sous-jacentes à ses actions\footcite{puren_intelligence_2020}. Cette ambition dépasse largement la simulation de l’intelligence humaine pour atteindre un niveau de fonctionnement identique à celui du cerveau humain, où la machine serait en mesure d’apprendre de manière autonome et d’adapter son comportement en fonction des nouvelles informations qu’elle acquiert. Cependant, ce type d’IA demeure largement hypothétique et soulève des défis colossaux.
En revanche, l’IA dite « faible » est moins ambitieuse, mais aussi plus réalisable. Elle désigne des programmes capables de raisonner, d’accumuler des informations et de résoudre des problèmes, mais en simulant seulement ces capacités, sans pour autant posséder une véritable « intelligence » \footcite{puren_intelligence_2020}. Cette approche est aujourd’hui celle qui sous-tend la majorité des systèmes d’intelligence artificielle disponibles, y compris ceux utilisés pour le traitement de texte, la reconnaissance vocale ou encore l’analyse d’image.
\newline
\textbfit{Machine learning et deep learning}\\

L’apprentissage automatique, ou \textit{machine learning}, constitue un domaine central de l’intelligence artificielle. Il s’agit de « l’étude et de l’entraînement d’algorithmes » permettant aux ordinateurs d’apprendre à partir de grandes quantités de données et de faire des prédictions. Cet apprentissage peut se faire de manière supervisée, c’est-à-dire avec l’intervention humaine, ou de manière non supervisée, où l’algorithme apprend de lui-même en identifiant des corrélations dans les données\footcite{puren_intelligence_2020}.
Dans l’apprentissage supervisé, l’algorithme est d’abord entraîné sur un ensemble de données fournies par des humains avant d’être appliqué à de nouvelles données non connues. La qualité des résultats dépend alors en grande partie de la qualité des données fournies et de la cohérence de l’entraînement. En revanche, l’apprentissage non supervisé implique une exploration autonome des données par l’algorithme, sans intervention humaine préalable. Ce dernier regroupe et classe les informations de manière autonome, sans avoir été spécifiquement programmé pour résoudre une tâche donnée\footcite{puren_intelligence_2020}.
L’essor du \textit{deep learning} depuis le milieu des années 2010 a représenté une avancée majeure dans le domaine de l’apprentissage automatique. Reposant sur des réseaux de neurones profonds, c’est-à-dire des architectures composées de plusieurs couches de neurones, chacune interprétant les informations de la couche précédente\footcite{puren_intelligence_2020}, ce type d’algorithme connaît de nos jours une certaine expansion en raison de la disponibilité accrue de vastes quantités de données d’entraînement et de l’accroissement des capacités de calcul des machines. En exploitant ces nouvelles ressources, les réseaux de neurones artificiels sont désormais capables d’apprendre de manière quasi autonome à partir des informations qu’ils traitent. 

\textbfit{Objectifs d’origine}\\

Le contexte d’émergence des premières tentatives pour l’élaboration d’une intelligence artificielle est marqué par la guerre froide et notamment la nécessité en découlant de traductions précises, notamment du russe vers l'anglais. L'importance de la traduction en cette période a orienté la recherche vers le traitement du langage humain par des machines. Le « Georgetown-IBM experiment », mené le 7 janvier 1954, illustre cette tendance. Il s'agissait de la première démonstration d'une machine capable de traduire, bien que de manière très limitée, seulement 250 mots et 49 phrases soigneusement choisies du russe vers l’anglais\footcite{puren_intelligence_2020}. Cet événement est souvent cité comme une étape fondatrice dans le domaine de la traduction automatique.
Quelques années plus tard, en 1968, le gouvernement américain crée la société Systran (System Translation), un projet encore plus ambitieux\footcite{puren_intelligence_2020}. Ces initiatives trouvent plus tard leur apogée dans l'émergence de la traduction statistique dans les années 1980, rendue possible par l'utilisation de grands corpus alignés bilingues. Ces développements ont façonné durablement les recherches en \tal.
Parmi les avancées significatives dans le domaine de l'intelligence artificielle (IA), l'introduction des réseaux de neurones convolutifs (\cnn) dans les années 1990 occupe une place prépondérante\footcite{puren_intelligence_2020}. Ces réseaux permettent d'assembler des informations en couches successives de « neurones », chaque couche recevant et interprétant les informations transmises par la couche précédente\footcite{puren_intelligence_2020}. Ce modèle supposément biomimétique a jeté les bases de nombreuses applications modernes, notamment dans la reconnaissance d'images et le traitement du langage. Toutefois, il convient de rappeler que l’image du « réseau de neurones » n’est qu’une analogie, comme le souligne \citeauthor{sabah_intelligence_2004}Gérard Sabah\footcite[paragraphe 87]{sabah_intelligence_2004}.  En effet, la connaissance actuelle du fonctionnement du cerveau humain est encore partielle, et l’analogie entre les réseaux de neurones artificiels et biologiques repose davantage sur une représentation simplifiée que sur une réalité scientifique.

\subsubsection{Courants symboliques et connexionnistes}

L'intelligence artificielle a connu des périodes de recherche marquées par deux approches dominantes : l'IA symbolique et l'IA connexionniste. Ces paradigmes s'opposent dans leurs principes fondamentaux : l'approche symbolique, ou cognitiviste, repose sur un raisonnement déductif et logique, empruntée au raisonnement humain conscient, tandis que l'approche connexionniste privilégie un raisonnement inductif, « basé sur l'expérience » \footcite[p.7]{bensamoun_rapport_2020}. Le \textit{deep learning}, appartenant au modèle connexionniste, marque le triomphe de cette approche, qui a pourtant connu une certaine mise à l’écart dans les décennies précédentes.

\textbfit{IA symbolique}\newline

L'IA symbolique, parfois surnommée « bonne vieille IA » (\textit{good old fashioned AI} ou GOFAI), se base sur des moteurs de règles et des systèmes experts. Cette approche repose sur un raisonnement formel, logique et explicable, rendant chaque étape du processus suivi par la machine compréhensible par l’humain. Parmi ses pionniers, on retrouve des chercheurs influents tels que Hubert Gelrnter, Allan Newell, Herbert Simon, John McCarthy, James Slagles et Thomas Evans qui ont contribué à poser les fondements d'une IA capable de manipuler des symboles et de résoudre des problèmes complexes par la mise en œuvre de règles prédéfinies\footcite{ezratty_que_nodate}.

\textbfit{IA connexionniste}\newline

En opposition à l'approche symbolique, le connexionnisme est fondé sur des réseaux neuronaux inspirés du fonctionnement biologique du cerveau humain. L'article fondateur de cette approche, intitulé « A logical calculus of the ideas immanent in nervous activity » (1943) et écrit par Warren McCulloch et Walter Pitts, marque un tournant majeur dans l'histoire de l'IA et l’acte de naissance du mouvement connexionniste selon Gilbert Saporta\footcite[p.3]{saporta_breve_2018}. Le \textit{perceptron} de Franck Rosenblatt en 1957-1958, algorithme d’entraînement supervisé spécialisé pour des tâches de classification linéaire (c’est-à-dire de division de données en « classes »), un réseau de neurones dit « simple » et présenté comme la première « machine à apprendre », et constitue l'une des premières implémentations formelles de cette approche\footcite[p.3]{saporta_breve_2018}. Le \textit{perceptron} et ses variantes plus complexes (comme les réseaux multicouches) sont des exemples classiques de réseaux de neurones artificiels, ancrés dans l’approche connexionniste, qui cherchent à simuler certains aspects des réseaux neuronaux « biologiques » (en s’appuyant sur le traitement distribué de l'information à travers des réseaux d'unités simples et interconnectées – comme des neurones).
Cette approche, bien que performante, est souvent critiquée pour son manque d'explicabilité, et ses « boîtes noires », rendant parfois difficile l'interprétation des résultats obtenus. Elle repose sur des méthodes probabilistes, notamment dans des domaines comme la reconnaissance d'images, où les solutions proposées se traduisent par des « pourcentages de véracité » plutôt que des certitudes formelles\footcite{ezratty_que_nodate}.

\textbfit{Opposition entre symbolistes et connexionnistes}\newline

L'opposition entre ces deux courants de l'IA s'illustre par la division entre les \textit{neats}, partisans des méthodes symboliques, et les \textit{scruffies}, adeptes des méthodes connexionnistes, comme le rappelle Olivier Ezratty\footcite{ezratty_que_nodate}. Alors que l'approche symbolique privilégie la logique formelle et l'explicabilité, le connexionnisme est souvent perçu comme une « boîte noire », difficile à déchiffrer. Cependant, ces deux courants ne sont pas totalement irréconciliables, comme le souligne Olivier Ezratty, qui fait une analogie entre les façons dont les humains et les machines apprennent. Ainsi, un humain peut apprendre à la fois par la lecture et l’apprentissage de règles formelles (symbolisme) et par l'expérience empirique (connexionnisme). Ces deux modes d'apprentissage sont complémentaires : l'un traite les flux d'information, tandis que l'autre exploite le stock de connaissances acquises\footcite{ezratty_que_nodate}.

\textbfit{Le connexionnisme peut-il parvenir à dépasser le \emph{semantic gap} ? }\newline

Une question majeure dans le domaine du traitement computationnel du langage naturel, reste à savoir si le connexionnisme peut combler le « \textit{semantic gap} », concept rappelé par Lev Manovich et désignant l'écart entre la manière dont un humain extrait et traite des informations et la manière dont une machine le fait\footcite[p.22]{manovich_data_2015}. Ce problème est particulièrement prégnant dans le domaine de la pragmatique, une composante de la linguistique traitant de l'interprétation contextuelle du langage, et qui demeure aujourd'hui l'une des parties les plus complexes à transposer en algorithmes\footcite[p.19]{yvon_petite_2007}, qu’ils soient d’origine symbolique ou connexionniste.

\textbfit{Hivers de l’IA}\newline

L'histoire de l'IA est ponctuée de périodes dites « hivers », où les recherches stagnent, voire sont abandonnées. Pour l'IA connexionniste, le premier hiver a eu lieu entre 1970 et 1980, suite à la démonstration des limites du \textit{perceptron}\footcite{saporta_breve_2018}. Un deuxième hiver intervient à partir de la fin des années 1980, en raison de l'insuffisance des capacités de calcul nécessaires pour développer des réseaux neuronaux plus complexes\footcite{saporta_breve_2018}.
L'IA symbolique, quant à elle, connaît également ses périodes de recul, notamment depuis une quinzaine d’années. L'une des principales raisons réside dans « l'inadéquation de ses méthodes pour le traitement du langage », champ de recherche majeur en IA, ainsi que dans la difficulté à « collecter et structurer [de manière exhaustive] les règles du savoir humain » \footcite{ezratty_que_nodate}.

\textbfit{IA générale et IA forte}\newline

Un autre sujet de débat concerne la distinction entre IA générale (AGI) et IA forte, souvent confondues. L’IA générale, qui ferait preuve de capacités d’adaptation et de raisonnement comparables à celles de l'intelligence humaine, est encore loin d’être réalisée. Les débats à ce sujet restent vifs, et il est probable que la solution réside dans une combinaison des approches connexionnistes et symboliques selon Olivier Ezrarry\footcite{ezratty_que_nodate}.

\subsubsection{Applications possibles dans les GLAM (Galleries, Libraries, Archive, Museums) et dans les institutions de recherche}

Les humanités numériques (HN) bénéficient de nouvelles méthodes de traitement des données, notamment grâce à la data science, qui devient un outil essentiel pour gérer les grandes volumétries d’informations auxquelles sont confrontées les institutions patrimoniales. Par exemple, dans les musées, bibliothèques et archives, l’utilisation de techniques avancées de traitement de données permet non seulement de traiter des volumes massifs d’informations\footcite{bermes_patrimoine_2020}, mais aussi d’aborder de manière plus nuancée des volumes plus restreints\footcite[p.29]{manovich_data_2015}. 
Le concept de \textit{distant viewing} s’inscrit dans cette perspective. Adapté du \textit{distant reading} de Franco Moretti, ce concept se concentre sur l'analyse à grande échelle d'objets picturaux, en permettant de traiter des ensembles d’images de manière collective, plutôt qu’individuelle. L’IA, combinée aux pratiques de la science des données, offre des possibilités sans précédent pour traiter des volumes massifs d’images, ce qui est devenu indispensable après les réflexes de numérisation massive engagés dès les années 1990 et par l’explosion des productions d’archives nativement numériques\footcite{arnold_distant_2019}.
Un autre exemple frappant de l'application de l'intelligence artificielle (IA) dans le domaine du patrimoine est la reconstruction d'œuvres d'art dégradées ou disparues, dont nous ne disposons que de photographies de mauvaises qualités, comme les œuvres citées par David Stork\footcite[p.685-87]{stork_how_2023}, grâce au \textit{deep learning}. Toutefois, il est important de noter que la qualité des résultats dépend fortement de l'entraînement du modèle et des compétences en histoire de l'art mobilisées lors de cette phase d’apprentissage. En effet, la machine peut produire une hypothèse statistiquement plausible, mais historiquement inexacte. Il s'agit d'un domaine où la collaboration entre ingénieurs et chercheurs devient alors essentielle pour garantir la pertinence du résultat obtenu. Le dialogue entre ces deux groupes est indispensable pour que les ingénieurs, souvent perçus comme responsables des aspects techniques, et les chercheurs, experts du contenu, puissent conjointement interpréter les résultats produits par l’IA. En effet, L’ingénieur seul, à moins d’être lui-même un spécialiste (« un historien [de l’art] programmeur », pour reprendre la phrase d’Emmanuel Le Roy Ladurie\footcite{lemny__2017}) ne peut saisir toutes les nuances complexes à prendre en compte.
Enfin, il convient également de noter que l'IA n'est pas l'unique facteur ayant entraîné des changements de paradigmes en histoire de l'art. Comme l'expliquent Aubry, Costner et James, l'IA est certes un vecteur de transformation, mais elle ne fait que s’inscrire dans une tendance plus large qui inclut l’évolution des technologies numériques et les pratiques de recherche associées\footcite[p.59]{aubry_artificial_2021}.

\subsection{Perspectives offertes par la « vision par ordinateur »}

\subsubsection{Traitement de l’image picturale}

Dans le cadre du traitement d’images picturales, l'intelligence artificielle permet d'extraire, d'identifier et de segmenter des éléments spécifiques au sein des œuvres d'art. Le processus commence par l'extraction des données visuelles d'une image, qui est ensuite soumise à un algorithme de reconnaissance, spécialement entraîné pour l’identification de certains motifs. Celui-ci est capable de détecter des formes, des couleurs et des motifs, permettant ainsi la classification des différents éléments picturaux. La segmentation joue un rôle clé dans ce traitement en divisant l’image en plusieurs segments, chacun représentant une composante spécifique, telle qu’un personnage ou un objet. 
Dans le cadre du projet \pense, une partie du corpus Karbowsky a été traitée par des méthodes de segmentation manuelle. Cependant, l’ambition à long terme est d’automatiser cette tâche (pour les corpus qui s’y prêtent !), afin d’accélérer le processus d’analyse d’un corpus particulièrement vaste. Cela permettrait de traiter non seulement un volume important d'images, mais également d’appliquer ces techniques à des corpus hétérogènes de manière plus systématique.

\subsubsection{Traitement de l’image comportant du texte}

L’application des techniques d’intelligence artificielle au traitement de l’image contenant du texte est une composante fréquemment retrouvée dans les projets d’édition numérique. Le développement de l'\ocr et de l'\htr a permis une amélioration significative de la transcription automatique des documents. Ce processus se décompose en plusieurs étapes : tout d’abord, la segmentation permet de localiser les blocs de texte sur une image. Ensuite, une analyse de la mise en page (organisation logique des blocs) peut être effectuée pour organiser ces segments. Enfin, la transcription automatique (basée sur la reconnaissance de pixels) convertit les caractères identifiés dans chaque bloc en texte numérique\footcite[p.3]{chague_htr-united_2022}. Ces tâches reposent sur des techniques d'apprentissage supervisé ou non-supervisé, nécessitant l'entraînement de modèles spécialisés à partir de larges volumes de données d'exemple. Il existe de fait une grande proximité technique entre l’\htr et l’\ocr, les deux technologies partageant des principes similaires\footcite[p.5]{biay_chaine_2022}. Enfin, signalons que d’autres applications voisines existent,  comme \textit{l’Optical Layout Recognition} (OLR) et \textit{l’Optical Musical Recognition} (OMR). L'OLR s’intéresse à l’identification des structures de mise en page au sein des documents visuels, tandis que l’OMR se concentre sur la reconnaissance de partitions musicales\footcite[p.91]{bermes_patrimoine_2020}.

\subsection{Applications possibles en traitement automatique de la langue}

Le traitement automatique de la langue (TAL) est une discipline de plus en plus centrale dans les projets d'édition numérique, particulièrement ceux qui visent à automatiser et enrichir l'analyse des corpus textuels. En termes simples, le TAL se concentre sur le développement d'algorithmes capables de traiter, d'analyser et de comprendre les langues humaines naturelles à travers des processus automatiques. Parmi les applications notables dans ce domaine, on retrouve des tâches comme la reconnaissance d'entités nommées, qui permet l'identification automatique de noms propres (personnes, lieux, organisations) dans un texte. Cela facilite grandement l'indexation automatique des données et la cartographie des relations entre différents éléments au sein des corpus, des outils présentant un intérêt non négligeable pour des éditions numériques dans le domaine des humanités numériques, ou dans une approche différente, pour les instruments de recherche archivistique comme cela été l’objet du projet \textit{NER4Archives} par exemple\footnote{Voir pour cela : https://hal.science/hal-03625734}. D'autres tâches plus traditionnellement rattachée à l’analyse linguistique, comme l'étiquetage grammatical (ou \textit{POS tagging}) et la lemmatisation, qui permettent respectivement de catégoriser les mots par leur fonction et de réduire les mots à leur forme « canonique », peuvent s’avérer utiles dans les processus de pré-traitement textuel pour les éditions numériques\footnote{Sur l’évolution du TAL vers l’IA, voir : \footcite[paragraphe 17]{leon_histoire_2015}}.

\subsubsection{Textométrie et analyse statistique de la langue}

\textbfit{Une approche ancienne}\newline

L’analyse statistique et quantitative de la langue est l'une des premières approches historiquement associées au développement des humanités numériques. Elle est souvent assimilée à l'\ia symbolique, en raison de sa dimension systématique et quantitative. Le travail de Roberto Busa est fréquemment cité comme l'un des premiers exemples d'application de méthodes computationnelles dans l'analyse des textes. En tant que précurseur des humanités numériques, Busa a employé des techniques lexicographiques pour établir des concordances dans les corpus, préfigurant en cela, selon Pierre Mounier, le concept de \textit{distant reading} de Franco Moretti\footcite[paragraphe 5]{mounier_ibm_2018}. Il paraît cependant anachronique de parler d’usage d’IA symbolique pour le travail qu’a mené Busa, puisque le concept n’était pas encore formulé (1956) lorsqu’il débuta en 1949 son entreprise computationnelle. Il semble cependant s’être appuyé sur un système basé sur des règles statistiques, système qui, en évoluant, a été rattaché au mouvement symbolique de l’intelligence artificielle. Comme l'a noté Mounier, bien que le concept d'IA symbolique n'ait pas encore été formulé à l'époque, le travail de Busa s'appuyait déjà sur un système de règles statistiques, anticipant ainsi certaines des méthodes modernes d'analyse des corpus.
\newline
\textbfit{La textométrie, une formalisation de l’approche statistique des textes}\\

La textométrie, qui constitue l’une des approches du traitement statistique des corpus textuels, s'est principalement développée en France à partir des années 1970, avec les travaux de Pierre Guiraud, Charles Muller et de Jean-Paul Benzécri. Il s’agit de quantifier et de visualiser les relations entre les mots et les textes\footcite{pincemin_quest-ce_2008}, permettant ainsi de repérer des patterns lexicaux ou structurels au sein des corpus. Les résultats de ces analyses permettent non seulement de produire des visualisations cartographiques des textes, mais aussi d'interpréter les données sous la forme de regroupements et de mises en évidence de termes clés à travers les corpus  \footnote{Voir également \footcite{pincemin_textometrie_2020}}. 
Malgré l'importance de cette discipline, son usage dans les éditions numériques (bien que théoriquement facilitée par la compatibilité de la plateforme TXM avec le format \tei par exemple\footcite{noauthor_txm_2019}), notamment dans le domaine de l'histoire de l'art, reste relativement limité. Comme l'a observé Emmanuel Château-Dutier, l'application de la textométrie et de l'analyse du discours dans les éditions numériques en histoire de l'art demeure encore à explorer ; situation pouvant partiellement s’expliquer par la relative moindre attention portée au texte en histoire de l’art, au bénéfice de l’image\footcite[p.86]{chateau-dutier_editions_2021}.
\newline
\textbfit{La lecture distante et le traitement de corpus massifs}\\

Un autre concept clé dans ce champ est celui de la « lecture distante » (ou \textit{distant reading}), proposé par Franco Moretti en 2000. Ce dernier a développé cette approche comme une alternative à la lecture dite « proche » (ou \textit{close reading}), qui privilégie une analyse approfondie d'un texte singulier. À l'inverse, la lecture distante consiste à analyser de vastes bases de données textuelles, souvent constituées de milliers de textes, dans le but de détecter des motifs récurrents (des \textit{patterns}) ou des structures « qui traversent les siècles et les frontières » \footcite{puren_intelligence_2020} afin de dégager des tendances littéraires sur le long terme. Pour Moretti, les méthodes classiques d'analyse littéraire souffrent d'un biais, en se concentrant exclusivement sur des auteurs appartenant au canon littéraire, excluant de fait une grande partie de la production textuelle de leur époque\footcite[paragraphe 12]{mounier_ce_2018} ; la lecture distante entend ainsi combler ce manque par l’analyse de corpus massifs, assistée de nos jours par méthodes computationnelles, qu’elles soient statistiques, issues de la textométrie ou d’autres approches voisines (la fouille de texte ou l’analyse sémantique latente, sur lesquelles nous ne nous attarderons pas ici), ou neuronales, par le recours aux capacités de classification d’un grand modèle de langage par exemple.

\subsubsection{La correction automatique de texte : de l’IA symbolique au \textit{deep learning}}

La correction de textes revêt une importance scientifique centrale dans le cadre d’une édition critique, particulièrement pour des projets visant à produire une version intelligible et fluide d’un texte en éliminant les scories qui en entravent la lecture, telles que les ratures, hésitations graphiques, ou encore erreurs variées. La tradition éditoriale critique tend à présenter un texte corrigé et « normalisé » selon les conventions orthographiques, grammaticales et de mise en forme contemporaines de l’époque de la publication\footcite{nougaret_ledition_2015}. Dans le cadre du projet \pense, nous nous sommes intéressés à la question de la post-correction orthographique et syntaxique du corpus de correspondance Doucet - René-Jean, successive à la production d’une transcription manuelle imitative ayant scrupuleusement respecté la forme d’origine, comprenant de nombreux écarts par rapport aux conventions d’écriture du XXIème siècle. Plus précisément, il s’agissait de s’interroger sur les possibilités offertes par un grand modèle de langage en ce qui concerne cette tâche de correction.
La correction automatique a suivi une évolution importante depuis ses débuts. Dans un premier temps, les correcteurs (ou « correcticiels » \footcite{jacquet-pfau_correcteurs_2001}) étaient basés sur des règles simples, telles que les correspondances exactes de mots, avant de se complexifier avec l’introduction de dictionnaires. Cela a permis d’élargir le champ des corrections, mais cette approche restait limitée aux erreurs directement identifiables par rapport aux normes lexicales et orthographiques. Le développement d’algorithmes de détection plus sophistiqués, comme ceux basés sur des modèles statistiques, a permis de franchir une nouvelle étape, notamment en prenant en compte les probabilités d’apparition des mots dans un contexte donné\footcite{mitton_fifty_2010}. Enfin, les dernières avancées dans le domaine de l’intelligence artificielle ont introduit le \textit{deep learning}, ayant recours à des réseaux neuronaux pour détecter des erreurs plus subtiles.

\textbfit{Usage d’un LLM pour la correction automatique de textes}\newline

Il existe une littérature relativement abondante sur l’usage de grands modèles de langage (\llm) pour la post-correction de texte bruités, c’est-à-dire présentant des anomalies essentiellement d’ordre typographiques, générés par \ocr ou \htr (reconnaissance automatique des caractères, typographiques ou manuscrits). Cependant, avant de plaquer les observations émises dans ces contextes-ci sur le cas de la post-correction que nous avons choisi d’opérer pour le corpus Doucet, il est important de noter une différence importante : le corpus textuel sur lequel nous travaillons est un corpus produit par transcription imitative générée par l’humain. Les anomalies ou erreurs caractéristiques rencontrés dans ce corpus sont donc de nature différente des anomalies rencontrées dans les sorties d’\ocr ou d'\htr. Là où un corpus imitatif reflétera les erreurs spécifiques d’un scripteur, pouvant avoir plusieurs origines (adoption d’une graphie fautive, erreur d’inattention, conventions orthographiques ou syntaxiques appartenant à une époque différente), les erreurs de transcription post-\ocr ou \htr se rapprochent davantage de fautes de frappe, dues à des confusions typographiques\footcite{mitton_fifty_2010}. Par exemple, un texte \ocr est souvent sujet à des erreurs où des caractères similaires sont confondus, comme la lecture d’un double N « nn » au lieu d’un « m » \footcite[p.95]{baranes_vers_2012}.

\textbfit{Fonctionnement d’un LLM pour le traitement du langage}\newline

Un \llm fonctionne très différemment des techniques de correction symbolique traditionnelles. La première étape dans le processus de correction est le prétraitement du texte. Ce prétraitement inclut la tokenisation (découpage du texte en unités distinctes appelées tokens) et la vectorisation desdits tokens. Cette étape consiste à transformer les tokens en vecteurs mathématiques, ce qui permet aux modèles de travailler sur des données numériques\footcite{das_building_2024}. Comme l’explique Ezratty, le \textit{deep learning} ne « comprend » pas le texte au sens humain du terme, mais il génère une représentation mathématique de ce texte, traitant les mots voire les syllabes comme des vecteurs\footcite{ezratty_que_nodate}.  Les calculs de distance entre vecteurs permettent alors de repérer des similarités entre des séquences textuelles.
La deuxième étape clé dans le fonctionnement d’un \llm, tel que GPT de OpenAI est la contextualisation du texte via le mécanisme d'attention. Ce processus repose (pour un \llm tel que GPT) sur une architecture appelée \textit{Transformer}, dans laquelle des couches d'attention (notamment l'attention multi-tête\footcite{noauthor_transformateurs_nodate}) permettent au modèle de « comprendre » le contexte global du texte. Ce mécanisme, appelé « self-attention », permet au modèle de peser l'importance de chaque mot dans une séquence en tenant compte des autres mots qui l’entourent\footcite[p.2]{vaswani_attention_2023}.   Cela donne au \llm une capacité à maintenir un contexte cohérent sur de longues distances dans le texte, contrairement aux approches plus locales des techniques symboliques.
La troisième étape est la génération de la correction, qui repose sur la prédiction probabiliste des tokens les plus appropriés à chaque position dans la séquence. Le modèle propose des substitutions, des réarrangements, ou même la suppression de certains tokens en fonction des probabilités calculées. La différence entre la génération de la correction et la reconstruction du texte réside dans le fait que la génération se concentre sur les changements locaux (à l’échelle du token ou de la séquence), tandis que la reconstruction consiste à valider le texte dans son ensemble pour garantir une cohérence structurelle.
L’utilisation des \llm pour la correction de corpus bruités, comme les textes historiques océrisés, a cependant ses limites. Certaines études indiquent que ces modèles ont tendance à surcorriger, c’est-à-dire à apporter des modifications inutiles ou exagérées. Ce problème peut néanmoins être relativement atténué grâce à des techniques de \textit{prompt engineering}qui permettent d’optimiser les exemples et les instructions donnés au modèle\footcite[p.135]{boros_post-correction_2024}. Comme l’indiquent plusieurs auteurs, il vaut mieux qu'un correcteur automatique sous-corrige plutôt qu'il ne sur-corrige, car la sur-correction altère l'intégrité du texte original, tandis que la sous-correction préserve davantage le texte en dépit de quelques erreurs\footcite[p.2]{baranes_vers_2012}.

\textbfit{Les alternatives aux LLM pour la post-correction côté neuronal : zoom sur les MLM et les modèles Seq2Seq}\newline

Les \mlm sont une autre catégorie de modèles qui peuvent être utilisés pour la correction de textes. Leur fonctionnement diffère des \llm en ce qu’ils s’appuient sur une technique appelée « \textit{masked language modeling}». Ce processus consiste à masquer certains tokens dans une séquence, sous la forme ‘[MASK]’ par exemple, et à demander au modèle de prédire ces tokens manquants en se basant sur le contexte environnant. Parmi les \mlm, citons par exemple CamemBERT, adaptation franco-française du BERT de Google\footcite{martin_camembert_2020}.   Cette approche permet au modèle de comprendre les relations contextuelles de manière plus fine\footnote{Sur le fonctionnement précis des \mlm, voir : Daniel Jurafsky et James H. Martin, « Masked Language Models », in \textit{Speech and Language Processing}, 2024, https://web.stanford.edu/~jurafsky/slp3/11.pdf}.   
Le \textit{Seq2Seq} (\textit{Sequence to Sequence}) est une architecture utilisée pour des tâches de génération de texte, notamment la traduction ou la correction automatique. Contrairement aux \llm, qui utilisent généralement un seul décodeur, les modèles \textit{Seq2Seq} reposent sur une architecture encodeur-décodeur, où l’encodeur transforme une séquence d'entrée en une représentation fixe, et le décodeur génère une nouvelle séquence à partir de cette représentation. Cette approche permet une vision plus globale du texte et une meilleure compréhension des relations entre les tokens, ce qui en fait un outil puissant pour la correction de textes\footcite{sutskever_sequence_2014}. La technique de vectorisation utilisée dans \textit{Seq2Seq}, est également différente de celle des \llm, car elle vise à créer des représentations continues des mots dans un espace vectoriel, en s’appuyant sur la similarité sémantique entre les mots et non seulement sur leur fréquence d'apparition dans les textes. Il semble qu’un modèle \textit{Seq2Seq} aura une vision plus « globale » que celle d’un LLM vis-à-vis du texte pris en entrée pour une tâche de correction. 